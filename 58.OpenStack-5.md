# OpenStack业务组件安装

## Glance

Glance是OpenStack的镜像服务，可以让用户上传、导出、修改、删除虚拟机镜像。另外，OpenStack镜像服务支持将镜像文件存储在各种类型的存储中，比如本地文件系统、OpenStack对象存储服务swift、ceph等分布式存储

OpenStack镜像服务包括glance-api、glance-registry两个子服务。glance-api对外提供通信接口，与其他服务交互；glance-registry用户管理存储在硬盘或glance数据库中的对象。修改配置后，需要重启openstack-glance-api.service或openstack-glance-registry.service

1. 在控制节点的OpenStack上配置Glance信息

```shell
# 1.创建Glance数据库
[root@controller ~]# mysql -uroot -predhat
MariaDB [(none)]> CREATE DATABASE glance default character set utf8;
MariaDB [(none)]> GRANT ALL PRIVILEGES ON glance.* TO 'glance'@'localhost' IDENTIFIED BY 'openstack';
MariaDB [(none)]> GRANT ALL PRIVILEGES ON glance.* TO 'glance'@'%' IDENTIFIED BY 'openstack';
MariaDB [(none)]> quit;

# 2.创建glance用户
[root@controller ~]# openstack user create --domain default --password openstack glance
[root@controller ~]# openstack role add --project service --user glance admin
[root@controller ~]# openstack service create --name glance --description "OpenStack Image" image

# 3.创建glance API服务端点
[root@controller ~]# openstack endpoint create --region RegionOne image public http://controller:9292
[root@controller ~]# openstack endpoint create --region RegionOne image internal http://controller:9292
[root@controller ~]# openstack endpoint create --region RegionOne image admin http://controller:9292
```

2. 控制节点安装Glance

```shell
# 1.安装Glance
[root@controller ~]# yum install -y openstack-glance
[root@controller ~]# cd /etc/glance && mv glance-api.conf glance-api.conf.source
[root@controller glance]# more glance-api.conf.source | egrep -v "^$|^#" > glance-api.conf
[root@controller glance]# chown root:glance /etc/glance/glance-api.conf

# 2.修改glance-api配置
[root@controller glance]# vim /etc/glance/glance-api.conf
[DEFAULT]		# 在[DEFAULT]部分，增加rabbitmq的配置
show_image_direct_url = True
transport_url = rabbit://openstack:openstack@controller

[glance_store]		# 在[glance_store]部分，配置使用哪种格式存储镜像，默认使用文件系统存储。也支持swift、ceph、GlusterFS
stores = file,http
default_store = file
filesystem_store_datadir = /var/lib/glance/images/

[keystone_authtoken]		# 在[keystone_authtoken]和[paste_deploy]部分，配置身份认证服务
www_authenticate_uri = http://controller:5000
auth_url = http://controller:5000
memcached_servers = controller:11211
auth_type = password
project_domain_name = Default
user_domain_name = Default
project_name = service
username = glance
password = openstack

[paste_deploy]
flavor = keystone

# 3.修改glance-registry配置
[root@controller glance]# mv glance-registry.conf glance-registry.conf.source
[root@controller glance]# more glance-registry.conf.source | egrep -v "^#|^$" > glance-registry.conf
[root@controller glance]# chown root:glance glance-registry.conf
[root@controller glance]# vim /etc/glance/glance-registry.conf
[DEFAULT]

[database]
connection = mysql+pymysql://glance:openstack@controller/glance

[keystone_authtoken]
www_authenticate_uri = http://controller:5000
auth_url = http://controller:5000
memcached_servers = controller:11211
auth_type = password
project_domain_name = Default
user_domain_name = Default
project_name = service
username = glance
password = openstack


[paste_deploy]
flavor = keystone

# 4.同步Glance数据库
[root@controller glance]# su -s /bin/sh -c "glance-manage db_sync" glance

# 5.启动Glance服务并设置自启动
[root@controller glance]# systemctl enable openstack-glance-api.service openstack-glance-registry.service
[root@controller glance]# systemctl start openstack-glance-api.service openstack-glance-registry.service

# 6.验证Glance
[root@controller glance]# tail -f /var/log/glance/*.log		# 需要打开两个SSH连接，此连接监控Glance日志变化
[root@controller ~]# source /etc/keystone/admin-openrc.sh		# 另起一个SSH连接
[root@controller ~]# wget http://download.cirros-cloud.net/0.4.0/cirros-0.4.0-x86_64-disk.img		# 下载测试镜像。cirros是一个裁剪过的镜像，用于OpenStack故障检测和测试
[root@controller ~]# openstack image create "cirros-0.4.0-x86_64" --file cirros-0.4.0-x86_64-disk.img --disk-format qcow2 --container-format bare --public		# 上传镜像到Glance。上传的镜像分私有镜像和公有镜像，私有镜像只有上传者才有权限使用；如果要设置成公有镜像，只需要加上--public参数即可

# 7.确认上传的镜像和属性
[root@controller ~]# openstack image list		# 或使用glance image-list命令
[root@controller ~]# openstack image show ${image_id}		# 或使用glance image-show ${image_id}命令
[root@controller ~]# openstack image set cirros-0.4.0-x86_64 --public		# 将镜像设置为公用镜像
[root@controller ~]# openstack image delete ${image_name}		# 删除镜像
```

## Neutron

OpenStack网络使用的是一个SDN（Software Defined Networking）组件，即Neutron，SDN是一个可插拔的架构，支持插入交换机、防火墙、负载均衡器等，这些都定义在软件中，从而实现对整个云基础设施的精细化管控。前期规划中有三个网卡，其中ens33作为外部网络（在OpenStack术语中，外部网络常被称为Provider网络），同时也用作管理网络，便于测试访问，生产环境建议分开；ens36作为租户网络，即vxlan网络；ens37作为ceph集群网络

OpenStack网络部署方式可选的有OVS和LinuxBridge。此处选择LinuxBridge模式，部署大同小异。Neutron服务需要在控制节点和计算节点都安装，同时，Neutron与Nova两者服务有交互，所以配置Neutron服务时还需要修改Nova服务的配置

1. 在控制节点的OpenStack上配置Neutron信息

```shell
# 1.创建neutron数据库
[root@controller ~]# mysql -uroot -predhat
MariaDB [(none)]> CREATE DATABASE neutron default character set utf8;
MariaDB [(none)]> GRANT ALL PRIVILEGES ON neutron.* TO 'neutron'@'localhost' IDENTIFIED BY 'openstack';
MariaDB [(none)]> GRANT ALL PRIVILEGES ON neutron.* TO 'neutron'@'%' IDENTIFIED BY 'openstack';

# 2.创建neutron用户
[root@controller ~]# source /etc/keystone/admin-openrc.sh
[root@controller ~]# openstack user create --domain default --password openstack neutron
[root@controller ~]# openstack role add --project service --user neutron admin
[root@controller ~]# openstack service create --name neutron --description "OpenStack Networking" network

# 3.创建neutron API服务端点
[root@controller ~]# openstack endpoint create --region RegionOne network public http://controller:9696
[root@controller ~]# openstack endpoint create --region RegionOne network internal http://controller:9696
[root@controller ~]# openstack endpoint create --region RegionOne network admin http://controller:9696
```

2. 控制节点安装Neutron

```shell
# 1.安装Neutron组件
[root@controller ~]# yum install -y openstack-neutron openstack-neutron-ml2 openstack-neutron-linuxbridge ebtables
[root@controller ~]# cd /etc/neutron/ && mv neutron.conf neutron.conf.source
[root@controller neutron]# more neutron.conf.source | egrep -v "^#|^$" > neutron.conf
[root@controller neutron]# chown root:neutron neutron.conf

# 2.修改neutron配置文件
[root@controller neutron]# vim /etc/neutron/neutron.conf
[DEFAULT]		# 在[DEFAULT]部分添加如下配置
core_plugin = ml2		# 启用Modular Layer 2（ML2）插件
service_plugins = router		# service_plugins默认值为空，router表示支持路由模式，即vxlan
transport_url = rabbit://openstack:openstack@controller
auth_strategy = keystone		# 设置验证策略是keystone
notify_nova_on_port_status_changes = true		# 通知计算节点的网络拓扑变化
notify_nova_on_port_data_changes = true
allow_overlapping_ips = true

[database]		# 配置数据库的访问
connection = mysql+pymysql://neutron:openstack@controller/neutron

[keystone_authtoken]		# 配置身份认证
www_authenticate_uri = http://controller:5000
auth_url = http://controller:5000
memcached_servers = controller:11211
auth_type = password
project_domain_name = default
user_domain_name = default
project_name = service
username = neutron
password = openstack
token_cache_time = 3600

[nova]		# 新增nova模块，通知计算节点网络拓扑变化，配置文件中默认没有此模块
auth_url = http://controller:5000
auth_type = password
project_domain_name = default
user_domain_name = default
region_name = RegionOne
project_name = service
username = nova
password = openstack

[oslo_concurrency]		# 配置锁定路径
lock_path = /var/lib/neutron/tmp
```

此时还没有安装Nova，Neutron配置文件中已经加入了nova模块的配置，未安装Nova的情况下配置文件中添加的nova模块的配置不会被使用，所以也不会报错

3. 修改插件配置

```shell
# 1.备份主配置文件
[root@controller neutron]# cd /etc/neutron/plugins/ml2/ && mv ml2_conf.ini ml2_conf.ini.source
[root@controller ml2]# more ml2_conf.ini.source | egrep -v "^#|^$" > ml2_conf.ini
[root@controller ml2]# mv linuxbridge_agent.ini linuxbridge_agent.ini.source
[root@controller ml2]# more linuxbridge_agent.ini.source | egrep -v "^$|^#" > linuxbridge_agent.ini
[root@controller ml2]# chown root:neutron ml2_conf.ini linuxbridge_agent.ini

# 2.修改插件配置文件
[root@controller ml2]# vim /etc/neutron/plugins/ml2/ml2_conf.ini
[DEFAULT]

[ml2]
type_drivers = flat,vlan,vxlan		# 启动flat、vlan、vxlan网络
tenant_network_types = vxlan		# 租户网络类型是vxlan
mechanism_drivers = linuxbridge,l2population		# 网络部署方式选择LinuxBridge，l2population可用于提升性能
extension_drivers = port_security

[ml2_type_flat]		# 将provider配置为flat网络
flat_networks = provider

[ml2_type_vlan]		# 将vlan配置为flat网络
network_vlan_ranges = provider

[ml2_type_vxlan]		# 配置vxlan网络识别的网络范围，取值范围在1~16777215之间
vni_ranges = 1:3000		# 取值范围设置为1~3000

[securitygroup]		# 开启安全组，提高安全效率
enable_ipset = true

# 3.修改LinuxBridge配置文件
[root@controller ml2]# vim /etc/neutron/plugins/ml2/linuxbridge_agent.ini
[DEFAULT]

[linux_bridge]		# 将Provider虚拟网络映射到物理网络（能与外网通信的网段）
physical_interface_mappings = provider:ens33

[vxlan]		# 启动vxlan叠加网络，如果要禁用vxlan，则设置为false，同时注释其他两个选项
enable_vxlan = true
local_ip = 10.168.59.20		# 每个控制和计算节点都需要添加一个vxlan专用网卡，并保证该网卡的IP能够在节点间互通
l2_population = true

[securitygroup]		# 启用安全组并配置LinuxBridge iptables驱动
enable_security_group = true
firewall_driver = neutron.agent.linux.iptables_firewall.IptablesFirewallDriver

# 4.修改内核参数，确保内核支持网桥过滤器
[root@controller ml2]# vim /etc/sysctl.conf
...
net.bridge.bridge-nf-call-iptables = 1
net.bridge.bridge-nf-call-ip6tables = 1
[root@controller ml2]# modprobe br_netfilter		# 加载网桥过滤器模块
[root@controller ml2]# sed -i '$amodprobe br_netfilter' /etc/rc.local		# 添加自启动
[root@controller ml2]# chmod +x /etc/rc.d/rc.local
```

local_ip要修改为计算节点实际使用的vxlan IP；provider网络使用的是ens33网口，部署时根据实际情况调整，provider网络可以理解为能与外网相通的网络，后面在创建Flat类型网络时`--provider-physical-network`要制定的时provider

4. 修改代理配置

```shell
# 1.配置DHCP代理
[root@controller ml2]# cd /etc/neutron/ && mv dhcp_agent.ini dhcp_agent.ini.source
[root@controller neutron]# more dhcp_agent.ini.source | egrep -v "^$|^#" > dhcp_agent.ini
[root@controller neutron]# chown root:neutron dhcp_agent.ini
[root@controller neutron]# vim /etc/neutron/dhcp_agent.ini
[DEFAULT]		# 配置LinuxBridge驱动接口、DHCP驱动，并启用隔离元数据，这样公共网络上的实例就可以通过网络访问元数据
interface_driver = linuxbridge
dhcp_driver = neutron.agent.linux.dhcp.Dnsmasq
enable_isolated_metadata = true

# 2.配置元数据代理
[root@controller neutron]# cd /etc/neutron/ && mv metadata_agent.ini metadata_agent.ini.source
[root@controller neutron]# more metadata_agent.ini.source | egrep -v "^$|^#" > metadata_agent.ini
[root@controller neutron]# chown root:neutron metadata_agent.ini
[root@controller neutron]# vim /etc/neutron/metadata_agent.ini
[DEFAULT]		# 配置元数据主机和共享密钥 
nova_metadata_host = controller
metadata_proxy_shared_secret = openstack

# 3.配置layer-3代理
[root@controller neutron]# cd /etc/neutron/ && mv l3_agent.ini l3_agent.ini.source
[root@controller neutron]# more l3_agent.ini.source | egrep -v "^$|^#" > l3_agent.ini
[root@controller neutron]# chown root:neutron l3_agent.ini
[root@controller neutron]# vim /etc/neutron/l3_agent.ini
[DEFAULT]
interface_driver = linuxbridge
external_network_bridge =
```

至此，Neutron网络服务的配置基本已经完成，此前提及过，Neutron与Nova要互通，所以现在还需要对Nova的配置文件进行修改，而修改Nova服务的配置文件的前提是先安装Nova服务

5. nova配置

```shell
# 1.安装nova
[root@controller neutron]# yum install -y openstack-nova-api openstack-nova_conductor openstack-nova-novncproxy openstack-nova-scheduler
[root@controller neutron]# cd /etc/nova/ && mv nova.conf nova.conf.source
[root@controller nova]# more nova.conf.source | egrep -v "^$|^#" > nova.conf
[root@controller nova]# chown root:nova nova.conf
[root@controller nova]# vim /etc/nova/nova.conf
[neutron]
url = http://controller:9696
auth_url = http://controller:5000
auth_type = password
project_domain_name = default
user_domain_name = default
region_name = RegionOne
project_name = service
username = neutron
password = openstack
service_metadata_proxy = true
metadata_proxy_shared_secret = openstack
```

6. 数据库同步

```shell
# 1.创建初始化网络
[root@controller nova]# ln -s /etc/neutron/plugins/ml2/ml2_conf.ini /etc/neutron/plugin.ini

# 2.同步neutron数据库
[root@controller nova]# su -s /bin/sh -c "neutron-db-manage --config-file /etc/neutron/neutron.conf --config-file /etc/neutron/plugins/ml2/ml2_conf.ini upgrade head" neutron

# 3.设置服务自启动
[root@controller nova]# systemctl enable neutron-server.service neutron-linuxbridge-agent.service neutron-dhcp-agent.service neutron-metadata-agent.service neutron-l3-agent.service
[root@controller nova]# systemctl restart neutron-server.service neutron-linuxbridge-agent.service neutron-dhcp-agent.service neutron-metadata-agent.service neutron-l3-agent.service
[root@controller nova]# systemctl status neutron-server.service neutron-linuxbridge-agent.service neutron-dhcp-agent.service neutron-metadata-agent.service neutron-l3-agent.service
```

### 计算节点安装Neutron

如果想把controller也作为计算节点，那在controller上也执行以下步骤

1. 安装组件

```shell
# 1.由于neutron与nova之间有相互调用配置，此处一起安装
[root@computer01 ~]# yum install openstack-nova-compute
[root@computer01 ~]# yum install -y openstack-neutron-linuxbridge ebtables ipset

# 2.修改neutron配置文件
[root@computer01 ~]# cd /etc/neutron && mv neutron.conf neutron.conf.source
[root@computer01 neutron]# cat neutron.conf.source | egrep -v "^#|^$" > neutron.conf
[root@computer01 neutron]# chown root:neutron neutron.conf
[root@computer01 neutron]# vim /etc/neutron/neutron.conf
[DEFAULT]		# 配置消息队列访问和身份认证服务
transport_url = rabbit://openstack:openstack@controller
auth_strategy = keystone

[keystone_authtoken]		# 配置身份认证信息
www_authenticate_uri = http://controller:5000
auth_url = http://controller:5000
memcached_servers = controller:11211
auth_type = password
project_domain_name = default
user_domain_name = default
project_name = service
username = neutron
password = openstack
token_cache_time = 3600

[oslo_concurrency]		# 配置锁定路径
lock_path = /var/lib/neutron/tmp
```

2. 创建虚拟网络

为实例建立layer2虚拟网络并处理安全组规则，将Flat网络和外部物理网络接口对应

```shell
# 1.修改ml2插件配置
[root@computer01 ~]# cd /etc/neutron/plugins/ml2/ && mv linuxbridge_agent.ini linuxbridge_agent.ini.source
[root@computer01 ml2]# more linuxbridge_agent.ini.source | egrep -v "^$|^#" > linuxbridge_agent.ini
[root@computer01 ml2]# chown root:neutron linuxbridge_agent.ini
[root@computer01 ml2]# vim /etc/neutron/plugins/ml2/linuxbridge_agent.ini
[linux_bridge]
physical_interface_mappings = provider:ens33

[vxlan]
enable_vxlan = true
local_ip = 10.168.59.31
l2_population = true

[securitygroup]
firewall_driver = neutron.agent.linux.iptables_firewall.IptablesFirewallDriver
enable_security_group = True

# 2.确保内核支持网桥过滤器
[root@computer01 ml2]# vim /etc/sysctl.conf
net.bridge.bridge-nf-call-iptables = 1
net.bridge.bridge-nf-call-ip6tables = 1
[root@computer01 ml2]# modprobe br_netfilter		# 临时添加网桥过滤器
[root@computer01 ml2]# sysctl -p
[root@computer01 ml2]# sed -i '$amodprobe br_netfilter' /etc/rc.local
[root@computer01 ml2]# chmod +x /etc/rc.d/rc.local

# 3.服务自启动
[root@computer01 ml2]# systemctl enable neutron-linuxbridge-agent.service
[root@computer01 ml2]# systemctl restart neutron-linuxbridge-agent.service
```

`[vxlan]`模块下的`local_ip`要修改成计算节点规划的vxlan IP，provider网络使用的是ens33网口，部署时根据实际情况调整，provider网络可以理解为能与外部互联网相通的网络，后面在创建Flat类型网络时，物理网络`--provider-physical-network`要指定的是provider

### 验证Neutron

```shell
# 1.创建一个Flat网络
[root@controller ~]# openstack network agent list 	# 查看neutron插件服务是否正常
[root@controller ~]# openstack network create --share --external --provider-network-type flat public --provider-physical-network provider	# 创建一个Flat网络

# 2.创建一个子网
[root@controller ~]# openstack subnet create --network public \
> --allocation-pool start=192.168.59.100,end=192.168.59.240 \
> --dns-nameserver 233.5.5.5 \
> --gateway 192.168.59.2 \
> --subnet-range 192.168.59.0/24 subnet_192.168.59

# 3.创建网络接口
[root@controller ~]# openstack port create --network public --fixed-ip subnet=subnet_192.168.59 ip-address=192.168.59.110

# 4.查看网络信息
[root@controller ~]# openstack network list
[root@controller ~]# openstack subnet list
[root@controller ~]# openstack port list

# 5.删除网络
[root@controller ~]# openstack port delete ip-address=192.168.59.110
[root@controller ~]# openstack subnet delete subnet_192.168.59
[root@controller ~]# openstack network delete public
	# 不要删除网络和子网
```

在`linuxbridge_agent.ini`文件中的`physical_interface_mappings=provider:ens33`，所以当前创建的flat类型的物理网络`--provider-physical-network`要指定为provider；删除网络存在先后顺序，即删除子网下的网络接口->删除子网->删除网络