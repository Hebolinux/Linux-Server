# 017.Nginx性能优化

## 性能优化概述

基于Nginx的性能优化首先需要对系统和Nginx当前的运行状态做一个信息收集，例如了解当前系统的结构和瓶颈，了解当前系统运行的业务、服务，单个服务能够支撑多大并发、最高瓶颈是多少、支持多少qps（每秒查询率）的访问请求，通过一些系统监控工具和压力测试工具，获取当前系统架构能够承受多少的请求和并发，基于此数据做相应的性能评估和优化方向

其次需要了解业务模式，每一个性能优化的目的都是为业务提供服务，所以需要了解业务接口的类型、系统层次化的结构，比如电商网站的抢购模式，只会在某一段时间内流量突增、比如Nginx所代表的角色，是代理、动静分离还是后端服务，针对不同的场景提供不同的优化方向

最后需要考虑性能和安全的对比，性能与安全在一定程度上是对立的，安全检测严密会对性能产生影响，过度追求性能会导致安全隐患，所以设计防火墙功能时必须平衡好两者的关系

## 压力测试工具

在系统业务量没有增长前就需要做好相应准备，以防范业务量突增带来的接口压力，因此对于业务接口的压力测试就显得非常重要，在业务上线之前就需要先对业务接口进行请求和并发的测试。管理员需要对系统能够承受的压力有一个评估，然后通过工具检测系统是否能够满足对应压力的需求

1. 安装ab压力测试工具

```shell
[root@web01 ~]# yum install -y httpd-tools
```

2. ab压测工具的使用方式

```shell
ab -n 200 -c 2 http://127.0.0.1/
    -n：总的请求次数
    -c：并发请求数
    -k：是否开启长链接
    -s：最大超时时间，默认30s
```
3. 安装tomcat

```shell
[root@web01 ~]# yum install -y java
[root@web01 ~]# wget --no-check-certificate https://dlcdn.apache.org/tomcat/tomcat-9/v9.0.73/bin/apache-tomcat-9.0.73.tar.gz
[root@web01 ~]# tar -xzf apache-tomcat-9.0.73.tar.gz
[root@web01 ~]# mv apache-tomcat-9.0.73 /usr/local/tomcat
[root@web01 ~]# /usr/local/tomcat/bin/startup.sh    # 启动tomcat
```

yum安装的java版本较低，只能配合tomcat9，tomcat10会因为java版本问题启动失败

4. 配置Nginx静态网站与tomcat动态网站环境

```shell
[root@web01 ~]# vim /usr/local/nginx/conf.d/ab.example.com.conf
server {
    server_name ab.example.com;
    listen 80;
    location / {
        root /usr/local/nginx/html/;
        index index.jsp index.html;
        try_files $uri @java_page;
    }
    location @java_page {
        proxy_pass http://127.0.0.1:8080;
    }
}
[root@web01 ~]# echo "Nginx Ab Load" > /usr/local/nginx/html/test7.html
[root@web01 ~]# echo "Tomcat Ab Load" > /usr/local/tomcat/webapps/ROOT/test7.html
```

5. Nginx站点压力测试

```shell
[root@web01 ~]# curl http://127.0.0.1/test7.html    # 查看当前是Nginx站点还是Tomcat站点
[root@web01 ~]# ab -n10000 -c200 http://127.0.0.1/test7.html
...
Server Software:        nginx/1.22.1
Server Hostname:        127.0.0.1
Server Port:            80

Document Path:          /test7.html
Document Length:        14 bytes

Concurrency Level:      200     请求并发数
Time taken for tests:   2.260 seconds       处理完所有请求所用的总时长
Complete requests:      10000       总请求数
Failed requests:        0       失败的请求数
Write errors:           0       
Total transferred:      2630000 bytes       总传输大小
HTML transferred:       140000 bytes        HTML传输字节大小
Requests per second:    4423.96 [#/sec] (mean)      QPS，每秒请求数，意味着每秒需要处理多少请求
Time per request:       45.208 [ms] (mean)      客户端请求服务端时，每个请求需要耗费的时间
Time per request:       0.226 [ms] (mean, across all concurrent requests)       服务端处理客户端请求时，每个请求的处理时间
Transfer rate:          1136.23 [Kbytes/sec] received       传输速率
...

# 将html文件移走后再测试
[root@web01 ~]# mv /usr/local/nginx/html/test7.html /opt/
[root@web01 ~]# ab -n10000 -c200 http://127.0.0.1/test7.html
...
Non-2xx responses:      10000       # 测试结果多出一行非200系列响应码，请求回应总数
...
```

6. Tomcat站点压力测试

在测试Tomcat站点前，必须将Nginx站点的html文件移走，否则用户请求仍会由Nginx解析

```shell
[root@web01 ~]# curl http://127.0.0.1/test7.html    # 确保此时是Tomcat站点反馈
[root@web01 ~]# ab -n10000 -c200 -k http://127.0.0.1/test7.html
Server Software:        nginx/1.22.1
Server Hostname:        127.0.0.1
Server Port:            80

Document Path:          /test7.html
Document Length:        15 bytes

Concurrency Level:      200
Time taken for tests:   70.185 seconds
Complete requests:      10000
Failed requests:        208
   (Connect: 0, Receive: 0, Length: 76, Exceptions: 132)
Write errors:           0
Total transferred:      2700000 bytes
HTML transferred:       150000 bytes
Requests per second:    142.48 [#/sec] (mean)
Time per request:       1403.696 [ms] (mean)
Time per request:       7.018 [ms] (mean, across all concurrent requests)
Transfer rate:          37.57 [Kbytes/sec] received

[root@web01 ~]# ss -an  # 在压力测试下查看端口信息
```

Tomcat站点测试需要设置超时时间或长链接，否则ab命令可能会因为超时报错。在长链接场景下会产生大量的TIME_WAIT状态，每个TIME_WAIT都会占用一个端口，在大量请求、高并发的情况下，站点可能会因为端口用尽而无法响应用户请求；对比Nginx和Tomcat的测试可以非常明显观察到，Nginx的静态资源响应速率远超Tomcat

## 系统性能优化

Linux中所有资源都以文件的形式存在，比如消息、共享内存、连接等，句柄可以理解为指向这些文件的指针。文件句柄会随着进程的调用频繁增加，系统默认文件句柄是有限制的，不能让一个进程无限制的调用，所以管理员需要限制每个进程和每个服务使用多大的句柄，此为必要的优化调整参数。文件句柄有3种设置方式：系统全局修改、用户局部修改、进程局部修改

```shell
[root@web01 ~]# ulimit -a   # 查看单个进程可打开的句柄上限
[root@web01 ~]# ulimit -a PID    # 查看某个进程的句柄上限
[root@web01 ~]# ulimit -n   # 查看系统设置的最大文件句柄数

# 针对root用户，soft仅提醒，hard限制，nofile打开最大文件数
[root@web01 ~]# vim /etc/security/limits.conf
root soft nofile 65535
root hard nofile 65535

# *代表所有用户
[root@web01 ~]# vim /etc/security/limits.conf
* soft nofile 65535
* hard nofile 65535
* - nofile 65535    # -代表soft和hard两者兼具

# 针对Nginx进程
[root@web01 ~]# vim /usr/local/nginx/conf.d/ab.example.com.conf
worker_rlimit_nofile 65535  # 在nginx核心模块加入此选项
```

使用lsof监控文件描述符数量

```shell
[root@web01 ~]# lsof -i :80 # 查看监听80端口的进程
[root@web01 ~]# lsof -p $(more /usr/local/nginx/nginx.pid) | wc -l  # 统计nginx打开的文件描述符

# ab压力测试的同时检测nginx的worker进程占用的文件描述符数量
[root@web01 ~]# ab -k -n 10000 -c 200 http://127.0.0.1/test7.html
[root@web01 ~]# lsof -p 663 | wc -l
```

调整内核参数，使time_wait状态端口复用

```shell
[root@web01 ~]# vim /etc/sysctl.conf
...
net.ipv4.tcp_tw_reuse = 1
net.ipv4.tcp_timestamps = 1 # 打开时间戳
...
[root@web01 ~]# sysctl -p   # 从指定文件中加载内核参数，若未指定文件则从/etc/sysctl.conf中加载；此命令也可用于查看管理员手动添加的内核参数
[root@web01 ~]# sysctl -a   # 查看系统设置的默认的内核参数
```

## 代理服务优化

通常Nginx作为代理服务负责转发用户请求，那么转发的过程中开启HTTP长链接，能够减少握手次数、降低服务器损耗

1. 长链接语法示例（应用层优化）

```
Syntax: keepalive connections;
Default: -
Context: upstream
This directive appeared in version 1.1.4
```

`connections`的值代表着连接到upstream服务器的*空闲长连接的最大数量*，它的主要作用是设定每个Nginx的**单个worker进程**对于upstream中的server的最大空闲连接数。keepalive指令并不会限制Nginx的**所有worker进程**能够开启的连接到upstream服务器的**连接总数**；如果这个值设置过大，会导致过多的空闲连接占满了upstream中的server资源，长时间的TCP连接容易导致系统资源无效占用、如果这个值设置过小，在高QPS场景会产生大量连接被生成再被抛弃的情况，也就是大量的TIME_WAIT。因此这个数值的设定需要根据worker进程数量来调整

2. 配置Nginx代理服务使用长链接方式

在正常配置upstream的情况下，代理与后端服务器是不会保持长链接的，此前代理篇章有一个参数`proxy_http_version 1.1;`修改http协议版本为1.1，此参数能够让代理节点连接后端节点保持长链接，但这个长链接是针对用户请求的

从HTTP协议的角度看，Nginx代理在这个过程中，对于客户端它扮演着HTTP服务器端的角色。而对于真正的服务器端（在nginx的术语中称为upstream）Nginx代理又扮演着HTTP客户端的角色，keepalive指令出现在版本1.1.4，如果此时代理节点想要长期与后端节点保持长链接，就需要在upstream区块下添加keepalive参数

```shell
[root@lb01 ~]# more /etc/nginx/conf.d/proxy_optimize.conf 
upstream optimize {
        server 172.16.1.7:8080;
        keepalive 32;
}

server {
        listen 80;
        server_name _;
        charset utf-8,gbk;
        location / {
                proxy_pass http://optimize;
                proxy_set_header Connection ""; # 消除“connection”头字段，connection字段通过浏览器调试模式下能够看到值
                include proxy_params;   # 此文件内的参数也是优化参数
        }
}
```

`keepalive 16`表示最大保持16个空闲长链接，一个keepalive连接上默认最大能够处理100个请求，可以通过keepalive_requests参数修改设置。长链接默认存在一个超时时间，在负载较低的情况下，某个长链接在超时时间内如果没有任何请求，服务端节点会向客户端（代理节点）发起长链接断开，并将该链接的状态修改为TIME_WAIT，而代理节点设置了keepalive参数后不会断开长链接，也就导致服务端节点上会最多维持16个TIME_WAIT状态链接，这个TIME_WAIT状态也不会一直维持，在一段时间后断开

[keeyalive说明](https://blog.51cto.com/u_14036860/2765120)

3. 对于fastcgi服务器，需要设置fastcgi_keep_conn以便保持长连接

```shell
[root@web01 ~]# vim /usr/local/nginx/conf.d/lnmp.example.com.conf 
upstream fastcgi_backend {
        server 127.0.0.1:9000;
        keepalive 8;
}

server {
        listen 80;
        server_name lnmp.example.com;
        root /usr/local/nginx/html/lnmp/;
        location / {
                index test.php;
        }
        location ~ \.php$ {
                fastcgi_pass fastcgi_backend;
                fastcgi_keep_conn on;   # fastcgi启用长链接
                fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name;
                include fastcgi_params;
        }
}
```

此处fastcgi语法看起来可能有些奇怪，此前fastcgi并没有与代理语法联用过，但keepalive选项只能写在upstream区块中。*scgi和uwscgi协议没有保持连接的概念，但无论是proxy、fastcgi、uwsgi协议都有cache缓存的功能*。proxy的cache只能缓存静态资源，fastcgi能够缓存动态资源，无论是那种缓存都需要根据实际情况考量是否开启，缓存数据保留在代理节点的本地磁盘中，在本地磁盘性能较弱的情况下，开启缓存甚至可能降低站点的运行效率

4. 扩展命令

keepalive_requests，设置通过一个keepalive连接提供的最大请求数。超出最大请求数后关闭连接

```
Syntax: keepalive_requests number;
Default: keepalive_requests 100;
Context: upstream
# 该指令出现在1.15.3版本中
```

keepalive_timeout，设置超时时间，在此期间与代理服务器的空闲keepalive连接将保持打开状态

```
Syntax: keepalive_timeout timeout;
Default: keepalive_timeout 60s;
Context: upstream
# 该指令出现在1.15.3版本中
```

## 静态资源优化

前文内容是对Nginx的应用服务优化，如代理、动态资源访问。当Nginx作为静态资源Web服务器，用于处理静态资源时传输非常高效，这里的静态资源指的是非Web服务端运行处理而生成的文件

|静态资源类型|种类|
|浏览器渲染|HTML、CSS、JS|
|图片文件|JPEG、GIF、PNG|
|视频文件|FLV、MP4、AVI|
|其他文件|TXT、DOC、PDF、...|

### 优化方向一：静态资源缓存

静态资源缓存指的是浏览器的缓存，浏览器缓存设置用于提高站点性能，例如新闻类网站，图片一旦发布，改动的可能是非常小的。如果此时图片能在用户的浏览器上长期缓存，势必能够提升用户访问站点的效率。浏览器有自身的缓存机制，它基于HTTP协议缓存机制实现，在HTTP协议中有很多头信息，实现浏览器的缓存就需要依赖特殊的头信息来与服务器进行特殊验证，如Expires(http/1.0);cache-control(http/1.1)

![浏览器缓存](https://www.z4a.net/images/2023/03/27/a22ad213dfd8ce48363e7ccd3741662e.png)

**浏览器缓存过期校验机制**

1. 浏览器请求服务器会先进行Expires、Cache-Control的检查，检查缓存是否过期，如果没有过期则直接从缓存文件向用户呈现站点页面
2. 如果缓存过期，首先检查是否存在etag。如果存在则客户端会向web服务器请求if-None-Match，与etag值进行对比，有服务器决策返回200还是304
3. 如果etag不存在，则进行last-Modified检查，客户端会向Web服务端请求if-Modified-Since，与last-Modified进行对比，由服务器角色返回200还是304

![浏览器缓存调用流程](https://www.z4a.net/images/2023/03/27/b08151b4a5591cb7e56376ef85cd6d1e.png)