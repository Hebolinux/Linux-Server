# MHA高可用

MHA属于物理故障的高可用

## MHA架构软件说明

### 节点规划

MHA的数据库节点必须至少是1主2从独立实例，它不支持单机多实例，MHA的管理节点最好是一台独立机器，在上例环境中为了节省机器，将从节点db03同时作为管理节点

### MHA构成

Manager工具：mha4mysql-manager-0.56-0.el6.noarch.rpm

```shell
masterha_check_ssh      	检查MHA的SSH配置状况
masterha_check_repl         检查MySQL复制状况
masterha_manger             启动MHA
masterha_check_status       检测当前MHA运行状态
masterha_stop				停止MHA
masterha_master_monitor     检测master是否宕机
masterha_master_switch      控制故障转移（自动或者手动）
masterha_conf_host          添加或删除配置的server信息
```

Node工具：mha4mysql-node-0.56-0.el6.noarch.rpm

```shell
#这些工具通常由MHA Manager的脚本触发，无需人为操作
save_binary_logs            保存和复制master的二进制日志 
apply_diff_relay_logs       识别差异的中继日志事件并将其差异的事件应用于其他的
purge_relay_logs            清除中继日志（不会阻塞SQL线程）
```

### MHA配置须知

1. 软链接

    MHA软件内指定好了执行命令的绝对路径，不经过环境变量，所以需要指定软链接，否则后续切换数据库时找不到命令会报错；首次安装时不会用到此软链接命令，所以即便软链接配置错误，MHA也能够安装成功

2. 免密互信

    主要用于数据补偿，考虑到突发主库宕机情况，如果此时`binlog`还没能完全传输到从库，MHA可能通过`scp`等网络传输方式将`binlog`输送到从库，这个过程中为了实现自动化，互信的配置是必须的

3. 账户

    mha：MHA有个专用的管理用户，用于监测数据库状态和节点心跳、截取数据库日志等

4. 管理端安装

    一般建议独单使用一台机器安装MHA管理端，也可以与node节点安装在同一个机器上，但建议尽量取排名靠后的从库节点，一是因为从库节点压力较小，二是因为主库宕机时排名靠前的从库优先顶底成为主库；**管理节点manager本身是无状态的，它承载的压力相对较小，且能够通过不同的配置文件同时区分、管理多套主从复制结构**

### MHA部署

1. 所有节点配置关键程序软链接（以db01为示例）

    MHA的运行脚本里使用的是绝对路径执行命令，所以需要软连接

    ```shell
    [root@db01 ~]# ln -s /usr/local/mysql/bin/mysqlbinlog /usr/bin/mysqlbinlog
    [root@db01 ~]# ln -s /usr/local/mysql/bin/mysql /usr/bin/mysql
    ```

2. db01节点配置免密互信

所有节点都需要配置免密登录，否则免密检测不成功

    ```shell
    [root@db01 ~]# \rm -r /root/.ssh
    [root@db01 ~]# ssh-keygen -t rsa
    [root@db01 ~]# mv /root/.ssh/id_rsa.pub /root/.ssh/authorized_keys
    [root@db01 ~]# scp -r /root/.ssh db02:/root
    [root@db01 ~]# scp -r /root/.ssh db03:/root
        #拷贝完成后测试各个节点间登录是否还需要输入密码
    ```
   
3. 所有节点安装软件包（以db01为示例）<a id="github_link"></a>

    MHA结构中所有节点都需要安装node包，管理节点还需要单独安装manager包

    ```shell
    #mha下载地址
    mha官网：https://code.google.com/archive/p/mysql-master-ha/
    github下载地址：https://github.com/yoshinorim/mha4mysql-manager/wiki/Downloads

    #安装mha节点包
    [root@db03 ~]# yum install -y perl-DBD-MySQL
    [root@db03 ~]# rpm -ivh /opt/mha4mysql-node-0.56-0.el6.noarch.rpm
    ```

4. 主库新建mha用户并安装管理端

    ```shell
    #主库创建mha专用用户
    db01 [(none)]>grant all privileges on *.* to 'mha'@'10.0.0.%' identified by 'mha';

    #db03节点安装manager包
    [root@db03 ~]# yum install -y perl-Config-Tiny epel-release perl-Log-Dispatch perl-Parallel-ForkManager perl-Time-HiRes
    [root@db03 ~]# rpm -ivh /opt/mha4mysql-manager-0.56-0.el6.noarch.rpm
    ```

    同一时间安装`perl-Log-Dispatch`和`perl-Parallel-ForkManager`包可能会找不到包，可以安装epel后再安装这两个包

5. 管理节点准备配置文件

    ```shell
    [root@db03 ~]# mkdir /etc/mha           #创建配置文件目录
    [root@db03 ~]# mkdir -p /var/log/mha/app1	#创建日志目录
    [root@db03 ~]# vim /etc/mha/app1.cnf    #编辑mha配置文件
    [server default]
    manager_log=/var/log/mha/app1/manager   #核心日志，基于运行过程中产生的故障日志
    manager_workdir=/var/log/mha/app1       #日志目录
    master_binlog_dir=/data/binlog          #主库binlog位置目录；所有主从节点的binlog都需要打开，且目录位置最好一致
    user=mha                                   
    password=mha                               
    ping_interval=2     #探测心跳的间隔时间，每2秒检测一次，共3次机会
    repl_password=redhat    #主从复制相关用户，这两个配置是考虑到主库宕机时，从库之间需要重新选举出新的主库
    repl_user=repl          #并相互构建主从，构建主从的过程中需要调用`change master to`，需要使用主从用户
    ssh_user=root       #检测互信
    [server1]           #自动检测主库
    hostname=10.0.0.51
    port=3306                                  
    [server2]           #主库宕机时，如果从库节点压力都比较平均，那么依据排名顺序选择从库成为新主库
    hostname=10.0.0.52
    port=3306
    [server3]
    hostname=10.0.0.53
    port=3306
    ```

6. mha状态检查

    ```shell
    [root@db03 ~]# masterha_check_ssh --conf=/etc/mha/app1.cnf
    [root@db03 ~]# masterha_check_repl --conf=/etc/mha/app1.cnf
    ```

7. 管理节点开启MHA

    ```shell
    [root@db03 ~]# nohup masterha_manager --conf=/etc/mha/app1.cnf --remove_dead_master_conf --ignore_last_failover < /dev/null > /var/log/mha/app1/manager.log 2>&1 &		#启动日志，与配置文件中定义的核心日志不同，这个日志仅记录启动过程中的一些故障日志
    [root@db03 ~]# masterha_check_status --conf=/etc/mha/app1.cnf
    [root@db03 ~]# masterha_stop --conf=/etc/mha/app1.cnf
    ```

    `remove_dead_master_conf`：主库宕机时，做完主从切换后，自动将故障节点从配置文件中去掉
    
    `ignore_last_failover`：忽略最后一次切换，`manager`的自我保护机制，默认情况下两次故障切换之间必须间隔固定时间，为了避免收到限制加此参数
   
关于nohup：在当shell中提示了nohup成功后，还需要按终端上键盘任意键退回到shell输入命令窗口，然后通过在shell中输入exit来退出终端；如果在nohup执行成功后直接点关闭程序按钮关闭终端的话，这时候会断掉该命令所对应的session，导致nohup对应的进程被通知需要一起shutdown，起不到关掉终端后调用程序继续后台运行的作用。也就是说**如果nohup执行成功后，直接关闭远程程序，会导致MHA进程被杀死**

补充：MHA第一遍部署时，本人使用db01节点作为管理节点，使用上述同样的步骤，配置到MHA状态检查时，repl检查出现故障。ssh检查正常，直接使用repl用户登录也正常，百度查询的结果大多都是已运行的MHA迁移出现故障，与我的情况不相符，可以确定是的主库是能够作为MHA管理节点的，所以将整个MHA结构推倒重来后，我怀疑是因为从库的`change master to`配置导致的错误

由于我是从上一章节GTID配置无缝衔接到MHA的，所以从库的`change master to`配置信息未作修改，其中我认为`MASTER_HOST='172.16.1.51'`这一点是最有可能影响到repl检测结果的疑点，在MHA中我使用10.0.0.0/24网段，主从复制的配置主库却是172.16.1.0/24网段。推倒重来的第二遍我是用db03节点作为MHA的管理节点，仍以db01作为主库，实践结果成功。这个疑点因为本人较懒，没有再重新做测试，先做记录

## MHA FailOver的过程（故障转移）

从主库宕机到业务恢复正常的处理过程称其为故障转移

1. 快速监控到主库宕机
    
    故障转移的前提是必须能够第一时间监控到主库是否发生了故障，所以MHA启动时，管理节点通过`masterha_manager`脚本启动`MHA-manager`，`MHA-manager`启动前，会自动检查ssh互信（`masterha_check_ssh`）和repl主从状态（`masterha_check_repl`），在上例安装MHA的过程中有手动检查这两个状态，`MHA-manager`启动后通过`masterha_master_monitor`脚本和配置文件中指定的`ping_interval`间隔时间对所有节点做状态检查，当`masterha_master_monitor`脚本探测主库默认3次无心跳后，认为主库宕机

    ```shell
    [root@master ~]# mysqladmin ping	#数据库ping测试
    ```

2. 选择新主库

    重新选主过程中有3种算法，分别根据*强制选主参数、各从库日志数据量、配置文件先后顺序*3种条件依次匹配选主。首先判断配置文件中是否存在强制选主的参数，其次依据从库中获取到的日志信息量的大小对比，获取到的日志更多的从库作为新主库，日志条件相同的情况下则根据manager的配置文件内写入的顺序生成新主库

    1. 读取配置文件中是否有强制选主的参数；默认情况下如果从库落后主库100M的`relay logs`，MHA判断日志量差异过大，恢复数据的时间过长，将不会选择该从库成为新主库，因为对此从库恢复需要花费较长时间，通过`check_repl_delay`参数设置忽略该从库的复制延时

        ```shell
        ...
        [server2]
        hostname=192.168.213.129
        port=3306
        candidate_master=1	#设置为候选主库，发生主从切换时优先将此从库切换为主库，但日志差异过大的情况下仍不会选取此节点
        check_repl_delay=0	#忽略复制延时检测，与上一参数联用时确保此节点成为主库
        ...
        ```

        关于`candidate_master`的应用场景，早期架构MHA不支持VIP时会通过Keepalived配合使用，MHA负责新主库的漂移、KA负责VIP的漂移，但是可能会出现VIP和新主库不在同一个节点上的场景，所以需要强制指定新主库节点；现在多地多中心场景下，会通过`candidate_master`选择离主库最近的地点的从库

    2. 自动判断所有从库的日志量，将最接近主库数据的从库作为新主
    3. 按照配置文件内的先后顺序进行选主

3. 数据补偿

    主从复制的数据默认是异步同步，那么当主库宕机时从库可能还没有完全同步主库的数据，此时则需要将主库数据拷贝同步到从库上恢复，需要尽可能多的将损失追回，使用GTID的方式处理数据补偿会更加高
    
    数据补偿分2种情况，通过判断主从SSH的联通性进行区分，能够正常连接时会判断主从节点之间的日志量的差异，调用`save_binary_logs`脚本，各个从节点立即从主库保存相对应的缺失部分的`binlog`并恢复，无法正常连接时调用`apply_diff_relay_logs`脚本，计算各个从库的`relaylog`差异，从日志量最多的从节点，将`relaylog`恢复到其他从库

4. 解除从库身份

    从库上配置了`change master to`，解除从库身份需要`stop slave`并重置`change master to`参数，将从库恢复成普通身份

5. 剩余从库和新主库构建主从关系

    修改剩余从库的主从配置参数，连接新主库，由新主库替代原主库的工作

6. 应用透明（vip）

    正常情况下选举出新主库后，主库的IP地址发生改变，需要修改业务网站的配置文件，重新声明主库的IP。在数据库的层面可以通过应用透明，在不更改业务网站的配置文件的情况下，自动将IP地址漂移到新主库上，类似于KeepAlived的作用

7. 故障提醒

### MHA应用透明

早期版本的MHA没有VIP功能，由于VIP的需求日渐增大才开发了VIP的接口，管理员可以通过脚本的方式实现VIP的漂移，VIP漂移的脚本通过<a href="#github_link">github</a>下载

1. 调整VIP脚本

    ```shell
    [root@db03 ~]# tar -xzf /opt/mha4mysql-manager-0.56.tar.gz -C /opt
    [root@db03 ~]# cp /opt/mha4mysql-manager-0.56/samples/scripts/master_ip_failover /usr/local/bin/
    [root@db03 ~]# vim /usr/local/bin/master_ip_failover
    ...
    my (
    $command, $ssh_user, $orig_master_host, $orig_master_ip,
    $orig_master_port, $new_master_host, $new_master_ip, $new_master_port
    );
    #############################添加内容部分#########################################
    my $vip = '10.0.0.55/24';									#指定vip的地址，对外提供服务的网段地址
    #my $brdc = '10.0.0.255';								#指定vip的广播地址
    #my $ifdev = 'ens2';										#指定vip绑定的网卡
    my $key = '1';												#指定vip绑定的虚拟网卡序列号；作用类似与一网卡多IP时，网卡配置文件名后面的序号，例如eth0:1
    my $ssh_start_vip = "/sbin/ifconfig ens2:$key $vip";		#代表此变量值为ifconfig ens33:1 192.168.59.188
    my $ssh_stop_vip = "/sbin/ifconfig ens2:$key down";		#代表此变量值为ifconfig ens33:1 192.168.59.188 down
    #my $exit_code = 0;											#指定退出状态码为0
    #my $ssh_start_vip = "/usr/sbin/ip addr add $vip/24 brd $brdc dev $ifdev label $ifdev:$key;/usr/sbin/arping -q -A -c 1 -I $ifdev $vip;iptables -F;";
    #my $ssh_stop_vip = "/usr/sbin/ip addr del $vip/24 dev $ifdev label $ifdev:$key";
    ##################################################################################
    GetOptions(
    ...
    ```

2. 修改MHA配置文件

    ```shell
    [root@db03 ~]# vim /etc/mha/app1.cnf
    [server default]
    master_ip_failover_script=/usr/local/bin/master_ip_failover
    ```

3. 手动在主库(db01)上添加VIP

    首次运行VIP功能前需要保证主库上的VIP是存在的，通过手动添加VIP的方式添加

    ```shell
    [root@db01 ~]# yum install -y net-tools
    [root@db01 ~]# ifconfig ens2:1 10.0.0.55/24
    ```

4. 重启MHA

    在重启MHA之前，需要检查保证master_ip_failover脚本具备可运行权限，某些情况下该脚本内可能含有一些中文注释，也需要将中文字符转换为unix字符

    ```shell
    [root@db03 ~]# yum install -y dos2unix
    [root@db03 ~]# dos2unix /usr/local/bin/master_ip_failover
    [root@db03 ~]# chmod +x /usr/local/bin/master_ip_failover

    #重启MHA
    [root@db03 ~]# masterha_stop --conf=/etc/mha/app1.cnf
    [root@db03 ~]# nohup masterha_manager --conf=/etc/mha/app1.cnf --remove_dead_master_conf --ignore_last_failover < /dev/null > /var/log/mha/app1/manager.log 2>&1 &
    [root@db03 ~]# masterha_check_status --conf=/etc/mha/app1.cnf
    ```

### 故障提醒

MHA属于“一次性”的高可用结构，一旦主库宕机，vip实现故障漂移后，MHA-manager进程会自动down掉，此时需要重新启动MHA-manager进程，其中邮件提醒的方式比较常用

```shell
#拷贝邮件脚本
[root@db03 ~]# cp /opt/mha4mysql-manager-0.56/samples/scripts/send_report /usr/local/bin/send

#编辑测试邮件脚本
[root@db03 ~]# yum install -y sendemail
[root@db03 ~]# vim /usr/local/bin/testpl.sh
#!/bin/bash
/usr/bin/sendemail -o tls=no -f hebo1248@163.com -t 1015792427@qq.com -s smtp.163.com:25 -xu 邮箱登录账号 -xp 邮箱授权码 -u "MHA Waring" -m "YOUR MHA BE FAILOVER" &> /tmp/sendmail.log
[root@db03 ~]# sh /usr/local/bin/testpl.sh  #测试邮件脚本是否能够正常运行

#编辑MHA配置文件
[root@db03 ~]# vim /etc/mha/app1.cnf
[server default]
report_script=/usr/local/bin/send

#重启MHA
[root@db03 ~]# masterha_stop --conf=/etc/mha/app1.cnf
[root@db03 ~]# nohup masterha_manager --conf=/etc/mha/app1.cnf --remove_dead_master_conf --ignore_last_failover < /dev/null> /var/log/mha/app1/manager.log 2>&1 &
```