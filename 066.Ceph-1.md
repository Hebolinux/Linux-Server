# 分布式存储 Ceph

## Ceph介绍

### 一、什么是ceph

ceph是一个统一的、分布式的存储系统，其设计初衷是提供较好的性能、可靠性和可扩展性

- “统一的”意味着可以仅凭ceph这一套存储系统，同时提供对象存储、块存储和文件系统存储三种功能，这极大的简化了不同应用需求下的部署和运维工作

- “分布式”代表着ceph真正实现了去中心化，理论上可以无限扩展集群系统的规模

传统集群架构下，集群规模增大，数据库的集群规模必然也要随之增大，这是集中式思想带来的弊端；ceph集群的数据共享完全通过算法，不存在数据库这个组件，是完全分布式的，但也因此，ceph需要大量CPU、内存资源

任何集群都追求的三大特点：

1. 性能：性能优化的最终目的是I/O
2. 可靠性：排除单点故障风险
3. 可扩展性：未来能够扩展集群规模

### 二、什么是块存储、文件存储、对象存储

#### 1. 块级

磁盘的最小读写单位为扇区，1个或多个连续的扇区组成一个block块，也叫物理块，是操作系统的最小读写单位。通过`sudo blockdev --getbsz /dev/sda2`命令可以查看分区block大小

#### 2. 文件级

文件是文件系统提供的功能，单个文件由一个或多个逻辑块组成，逻辑块之间是不连续分布的。逻辑块大于或等于物理块整数倍

物理块与文件系统之间的映射关系为：扇区->物理块->逻辑块->文件系统；文件级数据写入转换关系为：*用户缓冲区（fd,buffer,size）->逻辑文件->系统缓冲区->逻辑文件块->物理块*，多层的转换肯定是要消耗效率的，如果操作的是对象则可以直接省去多层的映射关系以提高效率

#### 3. 块存储、文件存储、对象存储

- 块存储：*为客户端提供一块裸盘，需要管理员执行分区、格式化制作文件系统*；块存储在数据写入转换关系中位于最底层，这意味着如果将块存储用作共享存储，可能会出现数据不能及时同步的问题，例如，用户通过Web01写入数据，Web01首先会写入缓存，如果此时用户从Web02上读取该数据，共享存储会读不到数据

- 文件存储：*为客户端提供一个文件夹，管理员可以直接操作文件*；文件存储的整个转换关系都在后端的存储设备，使用共享存储的客户端越多，后端存储设备的检索压力就越大，因此，这就限制了集群的规模不能太大

- 对象存储：客户端提供文件的元数据和真实数据，服务端根据元数据和内容生成文件并存储到硬盘中；对象存储解决了块存储和文件存储的弊端，它能够满足多服务器的共享数据的一致性，且由于其本身没有文件系统的概念，所以也没有文件检索压力。对象存储的数据分两部分：*元数据+内容*，客户端使用对象存储时，是通过服务端提供的URL地址提交元数据与内容

## Ceph系统的层次结构

自下而上可以将Ceph系统分为4个层次：

- 基础存储系统RADOS（Reliable,Autonomic,Distributed Object Store，即可靠的、自动化的、分布式的对象存储）
- 基础库LIBRADOS
- 高层应用接口：包括3个部分

	1. 对象存储接口：RADOS GW（Gateway）
	2. 块存储接口：RBD（Reliable Block Device）
	3. 文件存储接口：Ceph FS（Ceph File System）

- 应用层：给予高层接口或基础库librados开发的各种APP、主机、VM等诸多客户端

```
radow集群是ceph的服务端，依据高层接口封装的应用则是客户端
```

## 基础存储系统RADOS

### RADOS的子集群

Ceph的底层是Rados，而Rados由多个子集群构成

- 1. 若干块数据盘：一个Ceph存储节点上可以有一个或多个数据盘，每个数据盘上部署有特定的文件系统，例如xfs、ext4、btrfs。可以是一个分区当一个disk、一个raid当一个disk、一块硬盘当一个disk，一块盘当一个disk更稳定

	```
	# 1.btrfs（B-tree 文件系统）：功能强大，耗费资源高
	Oracel在2014年8月发布了btrfs的第一个稳定版，它支持许多高大上的功能，例如 透明压缩（transparent compression）、可写的COW快照（writable copy-on-write snapshots）、去重（deduplication）、加密（encryption），因此ceph建议用户在非关键应用上使用该文件系统
	# 2.xfs
	xfs和btrfs相较ext3/4而言，在高伸缩性数据存储方面具备优势
	```
- 2. OSD（Object Storage Device）集群：一个做好了文件系统的disk由一个OSD Deamon管理

	```
	1、负责控制数据盘上的文件读写操作，与client通信完成各种数据对象操作等
	2、负责数据的拷贝和恢复
	3、每个OSD守护进程监视它子集的状态，以及其他OSD的状态，并报告给Monitor
	```

在一个服务器上，一个数据盘对应一个OSD Daemon，而一个服务器上可以有多块数据盘，所以仅一台服务器上就会运行多个OSD Daemon，该服务称为OSD节点。一个CEPH集群中有n个OSD节点，综合下来，OSD集群由几十个到几万个OSD Daemon组成

- 3. MON（Monitor）集群：MON集群由少量的、数目为奇数个的Monitor Daemon组成，负责Ceph所有集群中的所有OSD状态的发现与记录。理论上，一个MON就能够完成这个任务，只不过多个守护进程组成的集群能够保证高可靠性

ceph集群的子集群：

- osd daemon：一个osd daemon就是一个套接字应用程序，唯一对应一块逻辑数据盘。一个逻辑上的数据盘可以由多个实际硬盘组成，例如一块机械盘+一块固态盘的两个分区
- pg：多个osd daemon可以组成一个pg，pg类似RAID1，它会将每份数据复制写入到不同的osd daemon中，这被称为副本机制，副本数建议设置为3
- object：所有数据写入pg之前都会被切分，切分的数据片被称为object，一个object应该被写入到哪个pg都由ceph的算法决定，能够保证尽可能的数据均匀写入

从object写入pg的过程中，为了保证数据能够均匀写入，ceph通过算法决策每一个object应该写入哪个pg，即便如此，仍会出现某些pg未被选中的情况。为了防止因为这种情况导致的数据写入不均，从pg到osd daemon的过程中，每个osd daemon也能够与多个pg组成对应关系，这意味着每个osd daemon能够从多个pg中接收数据写入数据盘

实际上ceph的数据平衡也无法做到真正的平衡，只能通过算法得到伪数据平衡。从object	选择pg的过程使用hash算法，从pg选择osd daemon的过程使用crush算法；ceph适用于数据量大或单个文件容量大的场景下，在数据量小的情况下，ceph无法保证数据平衡

```
object -> hash算法 -> pg -> crush算法 -> osd daemon
```

ceph官方建议，如果ceph集群预测长时间不会扩展，一个osd daemon属于100个pg，否则一个osd daemon属于200个pg；osd daemon与pg的绑定关系是通过cursh算法决定的，如果要指定绑定关系需要修改公式

ceph的逻辑单位：

1. pool（存储池）：创建存储池时需要指定该存储池中的pg个数，实际上pg也是在此时才被创建出来。创建pg需要用到cursh算法，而cursh算法又决定了pg与osd daemon的对应关系，因此，在客户端写入数据之前，pg与osd daemon的对应关系是已经确定的，但这个对应关系不是一成不变的；例如，磁盘损坏时，osd daemon也会减少，对应的pg的副本数也减少，而根据配置的剧本数（假设是3），ceph会自动从其他仍存在的osd daemon中再生成新的对应关系绑定此pg，以保证每个pg的副本数都为3
2. pg（归置组）：ceph以pg为单位分配数据，pg是分配数据的最小单位，一个pg内包含多个osd daemon