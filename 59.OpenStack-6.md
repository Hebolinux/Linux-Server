# OpenStack业务组件安装

## Nova

Nova是OpenStack中的计算服务。OpenStack种虚拟机实例（instance）生命周期都是由Nova服务来管理完成，包括实例创建、调度、删除等。Nova服务包含一系列组件，其中包括nova-api、nova-conductor、nova-scheduler、nova-novncproxy、nova-compute。Nova是一个非常重要的核心组件，其对应子模块非常多，配置也会比较复杂

- nova-scheduler：把nova-api调用请求映射为OpenStack将要调度哪个服务器来响应运行实例得请求，会根据诸如CPU架构、可用域、内存、负载等做出调度决策
- nova-api：对外提供API接口来管理内部基础设施，例如启动停止实例
- nova-conductor：nova-compute和数据库之间的一个组件，nova-conductor建立的初衷是基于安全考虑，避免nova-compute直接访问数据库
- nova-novncproxy：提供控制台服务，允许最终用户以vnc方式访问实例控制台，后续如果使用spice-server，则需要停止这个服务
- nova-compute：用于管理实例生命周期。通过消息队列接收请求，并承担操作工作

1. 在控制节点的OpenStack上配置Nova信息

```shell
# 1.创建Nova数据库
[root@controller ~]# mysql -uroot -predhat
MariaDB [(none)]> CREATE DATABASE nova_api default character set utf8;
MariaDB [(none)]> CREATE DATABASE nova default character set utf8;
MariaDB [(none)]> CREATE DATABASE nova_cell0 default character set utf8;

MariaDB [(none)]> GRANT ALL PRIVILEGES ON nova_api.* TO 'nova'@'localhost' IDENTIFIED BY 'openstack';
MariaDB [(none)]> GRANT ALL PRIVILEGES ON nova_api.* TO 'nova'@'%' IDENTIFIED BY 'openstack';

MariaDB [(none)]> GRANT ALL PRIVILEGES ON nova.* TO 'nova'@'localhost' IDENTIFIED BY 'openstack';
MariaDB [(none)]> GRANT ALL PRIVILEGES ON nova.* TO 'nova'@'%' IDENTIFIED BY 'openstack';

MariaDB [(none)]> GRANT ALL PRIVILEGES ON nova_cell0.* TO 'nova'@'localhost' IDENTIFIED BY 'openstack';
MariaDB [(none)]> GRANT ALL PRIVILEGES ON nova_cell0.* TO 'nova'@'%' IDENTIFIED BY 'openstack';

# 2.创建nova用户
[root@controller ~]# openstack user create --domain default --password openstack nova
[root@controller ~]# openstack role add --project service --user nova admin
[root@controller ~]# openstack service create --name nova --description "OpenStack Compute" compute

# 3.创建nova API端点
[root@controller ~]# openstack endpoint create --region RegionOne compute public http://controller:8774/v2.1
[root@controller ~]# openstack endpoint create --region RegionOne compute internal http://controller:8774/v2.1
[root@controller ~]# openstack endpoint create --region RegionOne compute admin http://controller:8774/v2.1
```

OpenStack Rocky版本需要手动添加Placement数据库，也就是说此前安装Placement组件时的所有操作，都需要在这里重复一边，在Stein版本以后Placement组件独立，Placement组件安装的所有操作都单独处理

2. 控制节点安装Nova

```shell
# 1.安装Nova
[root@controller ~]# yum install -y openstack-nova-api openstack-nova-conductor openstack-nova-novncproxy openstack-nova-scheduler		# 控制节点此前配置Neutron时已经操作过此步骤，可跳过

# 2.修改nova配置文件
[root@controller ~]# vim /etc/nova/nova.conf
[DEFAULT]		# 在[DEFAULT]部分添加如下配置
enabled_apis = osapi_compute,metadata
transport_url = rabbit://openstack:openstack@controller
my_ip = 192.168.59.20
use_neutron = true
firewall_driver = nova.virt.firewall.NoopFirewallDriver
allow_resize_to_same_host = true

[api]		# 配置身份认证服务
auth_strategy = keystone
token_cache_time = 3600

[api_database]		# 配置数据库访问
connection = mysql+pymysql://nova:openstack@controller/nova_api

[database]		# 配置数据库访问
connection = mysql+pymysql://nova:openstack@controller/nova

[glance]		# 配置glance服务的API地址
api_servers = http://controller:9292

[keystone_authtoken]		# 配置身份认证信息
auth_url = http://controller:5000/v3
memcached_servers = controller:11211
auth_type = password
project_domain_name = Default
user_domain_name = Default
project_name = service
username = nova
password = openstack
token_cache_time = 3600

[neutron]		# 启用元数代理并配置密码，此步骤在配置Neutron时已经配置，只需要检查确认
url = http://controller:9696
auth_url = http://controller:5000
auth_type = password
project_domain_name = default
user_domain_name = default
region_name = RegionOne
project_name = service
username = neutron
password = openstack
service_metadata_proxy = true
metadata_proxy_shared_secret = openstack

[oslo_concurrency]		# 配置锁定路径
lock_path = /var/lib/nova/tmp

[placement]		# 配置对placement服务的访问
region_name = RegionOne
project_domain_name = Default
project_name = service
auth_type = password
user_domain_name = Default
auth_url = http://controller:5000/v3
username = placement
password = openstack

[scheduler]		# 配置周期性发现计算节点间隔
discover_hosts_in_cells_interval = 180

[vnc]		# 配置VNC代理，使用控制节点的管理接口IP，客户端本地host要有controller的IP对应关系
enabled = true
server_listen = $my_ip
server_proxyclient_address = $my_ip
novncproxy_host = 0.0.0.0
novncproxy_port = 6080
novncproxy_base_url = http://controller:6080/vnc_auto.html

# 3.修改元数据代理
[root@controller ~]# vim /etc/neutron/metadata_agent.ini		# 此步骤在配置Neutron时已经配置，只需要检查确认
[DEFAULT]
nova_metadata_host = controller
metadata_proxy_shared_secret = openstack
```

3. 同步数据库

```shell
# 1.同步数据库
[root@controller ~]# su -s /bin/sh -c "nova-manage api_db sync" nova
[root@controller ~]# su -s /bin/sh -c "nova-manage cell_v2 map_cell0" nova
[root@controller ~]# su -s /bin/sh -c "nova-manage cell_v2 create_cell --name=cell1 --verbose" nova
8e49c1c1-5572-499d-a4ca-38a855cdd89e
[root@controller ~]# su -s /bin/sh -c "nova-manage db sync" nova

# 2.验证cell0和cell1是否正确注册
[root@controller ~]# su -s /bin/sh -c "nova-manage cell_v2 list_cells" nova

# 3.服务器自启动
[root@controller ~]# systemctl enable openstack-nova-api.service openstack-nova-scheduler.service openstack-nova-conductor.service openstack-nova-novncproxy.service
[root@controller ~]# systemctl start openstack-nova-api.service openstack-nova-scheduler.service openstack-nova-conductor.service openstack-nova-novncproxy.service
[root@controller ~]# systemctl status openstack-nova-api.service openstack-nova-scheduler.service openstack-nova-conductor.service openstack-nova-novncproxy.service
```

4. 计算节点安装Nova

```shell
# 1.安装Nova组件
[root@computer01 ~]# yum install -y qpid-proton-c-0.26.0-2.el7.x86_64		# 这个包是openstack-nova-compute的依赖包，如果直接安装openstack-nova-compute没有依赖包报错，可省略这一步
[root@computer01 ~]# yum install -y openstack-nova-compute
[root@computer01 ~]# cd /etc/nova/ && mv nova.conf nova.conf.source
[root@computer01 nova]# more nova.conf.source | egrep -v "^$|^#" > nova.conf
[root@computer01 nova]# chown root:nova nova.conf

# 2.修改nova配置文件
[root@computer01 nova]# vim /etc/nova/nova.conf
[DEFAULT]		# 配置计算和元数据API、RabbitMQ消息队列
enabled_apis = osapi_compute,metadata
transport_url = rabbit://openstack:openstack@controller
my_ip = 192.168.59.31
use_neutron = true
firewall_driver = nova.virt.firewall.NoopFirewallDriver

[api]		# 身份认证服务
auth_strategy = keystone

[glance]		# glance服务API
api_servers = http://controller:9292

[keystone_authtoken]		# 身份认证信息
auth_url = http://controller:5000/v3
memcached_servers = controller:11211
auth_type = password
project_domain_name = Default
user_domain_name = Default
project_name = service
username = nova
password = openstack
token_cache_time = 3600

[libvirt]		# 配置虚拟化类型
virt_type = kvm		# 虚拟机模拟OpenStack可能需要将virt_type设置为qemu，否则创建虚拟机后，一直停在GRUB Loading stage2
num_pcie_ports = 10		# num_pcie_ports是虚拟机的pci数量，最多支持设置28个

[neutron]		# neutron访问参数
url = http://controller:9696
auth_url = http://controller:5000/v3
auth_type = password
project_domain_name = default
user_domain_name = default
region_name = RegionOne
project_name = service
username = neutron
password = openstack

[oslo_concurrency]		# 设置锁定路径
lock_path = /var/lib/nova/tmp

[placement]		# placement服务API
region_name = RegionOne
project_domain_name = Default
project_name = service
auth_type = password
user_domain_name = Default
auth_url = http://controller:5000/v3
username = placement
password = openstack

[scheduler]		# 配置周期性发现计算节点间隔
discover_hosts_in_cells_interval = 180

[vnc]		# 启用并配置远程访问控制台
enabled = true
server_listen = 0.0.0.0
server_proxyclient_address = $my_ip
novncproxy_base_url = http://controller:6080/vnc_auth.html
vncserver_proxyclient_address = $my_ip
```

此配置文件中`virt_type`仍然设置为`kvm`的原因是，此前在VMware上创建虚拟机时，勾选了虚拟化引擎的`Intel VT-x`，然后在CentOS上又配置了CPU嵌套虚拟化，所以`virt_type`可以设置为`kvm`

5. 计算节点安装KVM组件

```shell
# 1.安装KVM
[root@computer01 nova]# yum install -y qemu-kvm-common-ev qemu-kvm-tools qemu-kvm-ev libvirt-daemon-kvm qemu-guest-agent qemu-img-ev

# 2.修改libvirt配置文件
[root@computer01 nova]# vim /etc/libvirt/libvirtd.conf
listen_tls = 0
listen_tcp = 1
tcp_port = "16509"
listen_addr = "192.168.59.31"		# 不同的计算节点IP需要修改
auth_tcp = "none"

[root@computer01 nova]# vim /etc/sysconfig/libvirtd
LIBVIRTD_ARGS="--listen"

# 3.服务自启动
[root@computer01 nova]# systemctl enable libvirtd.service openstack-nova-compute.service
[root@computer01 nova]# systemctl restart libvirtd.service openstack-nova-compute.service
[root@computer01 nova]# ss -atnp | grep 16509		# 查看libvirtd是否正常
[root@computer01 nova]# more /var/log/nova/nova-compute.log		# 查看日志是否有报错
```

6. 控制节点验证Nova

```shell
# 1.查看OpenStack计算组件状态
[root@controller ~]# source /etc/keystone/admin-openrc.sh
[root@controller ~]# openstack compute service list		# 查看计算服务组件状态
[root@controller ~]# openstack compute service list --service nova-compute		# 查看已经注册的计算节点
[root@controller ~]# openstack catalog list		# 列出keystone服务种的API端点以验证与Identity服务的连接

```

执行`openstack compute service list --service nova-compute`可能会遇到以下报错：

```shell
[23:35:07]The server is currently unavailable. Please try again at a later time.<br /><br />
[23:35:07]The Keystone service is temporarily unavailable.
```

原因是创建nova用户时没有给nova用户授予admin角色，补充操作`openstack role add --project service --user nova admin`即可；在增加新的计算节点时，需要在控制节点手动执行命令以发现新的计算节点`su -s /bin/sh -c "nova-manage cell_v2 discover_hosts --verbose" nova`，如果已经配置了`discover_hosts_in_cells_interval = 180`，则会周期性注册新增的计算节点，不需要手动执行发现命令